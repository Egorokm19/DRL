{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: Реализация DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном задании вам будет необходимо реализовать алгоритм DQN или одну из его модификаций. Реализованный алгоритм необходимо использовать для того, чтобы научиться решать задачу LunarLander."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Можно запустить: ```python train.py``` в cmd или данный файл***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Установка необходимых модулей:\n",
    "* !pip install gym\n",
    "* !pip install box2d\n",
    "* !pip install torch\n",
    "* !pip insyall pyvirtualdisplay *если запустить на линукс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gym import make\n",
    "from random import sample\n",
    "from collections import deque\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from agent import Agent\n",
    "\n",
    "# если запустить на линукс pyvirtualdisplay отлично выводит процесс обученной модели\n",
    "# from pyvirtualdisplay import Display\n",
    "# display = Display(visible=0, size=(1400, 900))\n",
    "# display.start()\n",
    "\n",
    "# is_ipython = 'inline' in plt.get_backend()\n",
    "# if is_ipython:\n",
    "#     from IPython import display\n",
    "\n",
    "# plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим константные переменные\n",
    "LEARNING_RATE = 5e-4\n",
    "CONST_VAL = 0\n",
    "GAMMA = 0.99\n",
    "STEPS_PER_UPDATE = 4\n",
    "BATCH_SIZE = 128\n",
    "STATE_UNITS = 256\n",
    "INITIAL_STEPS = 1024\n",
    "TRANSITIONS = 500000\n",
    "STEPS_PER_TARGET_UPDATE = STEPS_PER_UPDATE * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализуем класс буфера памяти\n",
    "class Buffer:\n",
    "    def __init__(self):\n",
    "        self.buffer = deque(maxlen=10000)\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        return sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализуем класс DQN\n",
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.steps = CONST_VAL # Do not change\n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        # defining the buffer\n",
    "        self.buffer = Buffer()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, STATE_UNITS),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(STATE_UNITS, STATE_UNITS),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(STATE_UNITS, STATE_UNITS),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(STATE_UNITS, action_dim),\n",
    "        ) # Torch model\n",
    "        # device run\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # detected target\n",
    "        self.target = copy.deepcopy(self.model).to(self.device)\n",
    "        # optimizer\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        # loss mse\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def consume_transition(self, transition):\n",
    "        # Add transition to a replay buffer.\n",
    "        # Hint: use deque with specified maxlen. It will remove old experience automatically.\n",
    "        self.buffer.add(transition)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        # Sample batch from a replay buffer.\n",
    "        # Hints:\n",
    "        # 1. Use random.randint\n",
    "        # 2. Turn your batch into a numpy.array before turning it to a Tensor. It will work faster\n",
    "        batch = self.buffer.sample(self.batch_size)\n",
    "        return list(zip(*batch))\n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        # Use batch to update DQN's network.\n",
    "        state, action, next_state, reward, done = batch\n",
    "        state = torch.tensor(np.array(state), dtype=torch.float32)\n",
    "        next_state = torch.tensor(np.array(next_state), dtype=torch.float32)\n",
    "        reward = torch.tensor(np.array(reward), dtype=torch.float32).view(-1)\n",
    "        done = torch.tensor(np.array(done), dtype=torch.bool)\n",
    "        action = torch.tensor(np.array(action), dtype=torch.int64).view(-1, 1)\n",
    "        # target network            \n",
    "        with torch.no_grad():\n",
    "            q_target = self.target(next_state).max(dim=-1)[0]\n",
    "            q_target[done] = 0\n",
    "            q_target = reward + self.gamma * q_target\n",
    "        q_func = self.model(state).gather(1, action.reshape(-1, 1))\n",
    "        # calculate loss\n",
    "        loss = self.loss(q_func, q_target.unsqueeze(1))\n",
    "        # step\n",
    "        self.optimizer.zero_grad()\n",
    "        # calculate loss\n",
    "        loss.backward()\n",
    "        # step optimizer\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        # Update weights of a target Q-network here. You may use copy.deepcopy to do this or \n",
    "        # assign a values of network parameters via PyTorch methods.\n",
    "        self.target = copy.deepcopy(self.model)\n",
    "\n",
    "    def act(self, state, target=False):\n",
    "        # Compute an action. Do not forget to turn state to a Tensor and then turn an action to a numpy array.\n",
    "        state = np.array(state)\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            action = self.model(state).numpy()\n",
    "        return np.argmax(action)\n",
    "\n",
    "    def update(self, transition):\n",
    "        # You don't need to change this\n",
    "        self.consume_transition(transition)\n",
    "        if self.steps % STEPS_PER_UPDATE == 0:\n",
    "            batch = self.sample_batch()\n",
    "            self.train_step(batch)\n",
    "        if self.steps % STEPS_PER_TARGET_UPDATE == 0:\n",
    "            self.update_target_network()\n",
    "        self.steps += 1\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.model, \"agent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для загрузки политики игры LunarLander и определение параметров\n",
    "def evaluate_policy(agent, episodes=5):\n",
    "    env = make(\"LunarLander-v2\")\n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        total_reward = 0.\n",
    "        \n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(agent.act(state))\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация среды и запуск обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000, Reward mean: -124.5487814717917, Reward std: 96.09787632643042\n",
      "Step: 10000, Reward mean: -172.86712852131637, Reward std: 127.55505976734356\n",
      "Step: 15000, Reward mean: -118.23815419416333, Reward std: 94.31840788106732\n",
      "Step: 20000, Reward mean: -94.51355880589674, Reward std: 181.8690414130671\n",
      "Step: 25000, Reward mean: -140.62212791323154, Reward std: 57.889198802110506\n",
      "Step: 30000, Reward mean: -131.39738129234698, Reward std: 96.50820092711625\n",
      "Step: 35000, Reward mean: -118.72250988243937, Reward std: 23.315305513768184\n",
      "Step: 40000, Reward mean: -124.81965151859636, Reward std: 31.07349985996088\n",
      "Step: 45000, Reward mean: -120.7967659052812, Reward std: 11.6290736116102\n",
      "Step: 50000, Reward mean: -113.17326180933378, Reward std: 38.04921890799007\n",
      "Step: 55000, Reward mean: -92.9214794204257, Reward std: 4.72369479560845\n",
      "Step: 60000, Reward mean: -167.60146481222384, Reward std: 76.37784041434708\n",
      "Step: 65000, Reward mean: -147.3683522832108, Reward std: 56.873387840729436\n",
      "Step: 70000, Reward mean: -156.60562691366334, Reward std: 71.44272385522481\n",
      "Step: 75000, Reward mean: -105.99909213181918, Reward std: 39.768160561434144\n",
      "Step: 80000, Reward mean: -78.79705443979432, Reward std: 29.136423541896146\n",
      "Step: 85000, Reward mean: -61.52964305424674, Reward std: 26.66777786398328\n",
      "Step: 90000, Reward mean: -98.79699289190356, Reward std: 27.236038656297595\n",
      "Step: 95000, Reward mean: -66.07596419426781, Reward std: 22.435038677263524\n",
      "Step: 100000, Reward mean: -66.63237388914425, Reward std: 14.519475290832473\n",
      "Step: 105000, Reward mean: 0.47569300550531324, Reward std: 114.1285970854261\n",
      "Step: 110000, Reward mean: -63.39083561331002, Reward std: 44.89476876825212\n",
      "Step: 115000, Reward mean: -10.804792001560624, Reward std: 102.22075207671398\n",
      "Step: 120000, Reward mean: -42.13327470774861, Reward std: 37.78350462962205\n",
      "Step: 125000, Reward mean: -71.38028450457053, Reward std: 26.053633952330603\n",
      "Step: 130000, Reward mean: -44.72354343979217, Reward std: 27.94960083982799\n",
      "Step: 135000, Reward mean: -60.86056001667955, Reward std: 40.57188550542954\n",
      "Step: 140000, Reward mean: -46.75736761000152, Reward std: 35.112833838428564\n",
      "Step: 145000, Reward mean: -49.6693112732808, Reward std: 53.82263661123462\n",
      "Step: 150000, Reward mean: -34.78275143387589, Reward std: 31.173613928674616\n",
      "Step: 155000, Reward mean: -36.51765203186614, Reward std: 20.830594284082654\n",
      "Step: 160000, Reward mean: -64.57894189036148, Reward std: 43.738436625161484\n",
      "Step: 165000, Reward mean: -40.77931609072532, Reward std: 32.99406595456737\n",
      "Step: 170000, Reward mean: -26.30921574180946, Reward std: 30.142921885074383\n",
      "Step: 175000, Reward mean: 3.4501732724602916, Reward std: 44.36666103335679\n",
      "Step: 180000, Reward mean: -56.796838940406374, Reward std: 31.902332046272186\n",
      "Step: 185000, Reward mean: 8.639227840314827, Reward std: 53.713159889398504\n",
      "Step: 190000, Reward mean: -21.398925356577514, Reward std: 14.311308228121487\n",
      "Step: 195000, Reward mean: 24.162823543306228, Reward std: 84.06815748989486\n",
      "Step: 200000, Reward mean: -40.96003378153722, Reward std: 14.266009269214392\n",
      "Step: 205000, Reward mean: -11.119346679570612, Reward std: 58.443785035548274\n",
      "Step: 210000, Reward mean: -64.00112018521456, Reward std: 46.17466639618116\n",
      "Step: 215000, Reward mean: 128.84044043153992, Reward std: 103.17302057153337\n",
      "Step: 220000, Reward mean: 98.31658415697183, Reward std: 187.51723782760516\n",
      "Step: 225000, Reward mean: -51.5113781003062, Reward std: 53.04266659706232\n",
      "Step: 230000, Reward mean: -55.200283667518235, Reward std: 12.127537626754318\n",
      "Step: 235000, Reward mean: 141.19243518737454, Reward std: 137.23663272188278\n",
      "Step: 240000, Reward mean: 144.09332099928574, Reward std: 124.96092077055864\n",
      "Step: 245000, Reward mean: -33.54470110773825, Reward std: 151.53680524397947\n",
      "Step: 250000, Reward mean: 125.82368035484451, Reward std: 140.9181712759657\n",
      "Step: 255000, Reward mean: 67.59082891277305, Reward std: 211.33994828251764\n",
      "Step: 260000, Reward mean: 122.5173242716623, Reward std: 138.92018047106797\n",
      "Step: 265000, Reward mean: 133.00816929270388, Reward std: 136.16330778346446\n",
      "Step: 270000, Reward mean: 149.28610689474166, Reward std: 149.66317999374596\n",
      "Step: 275000, Reward mean: 168.67759308889057, Reward std: 131.43247386969009\n",
      "Step: 280000, Reward mean: 205.76864301372925, Reward std: 92.33828887060953\n",
      "Step: 285000, Reward mean: 112.58284585773893, Reward std: 142.08542591540157\n",
      "Step: 290000, Reward mean: 129.1035395080535, Reward std: 140.8628287082366\n",
      "Step: 295000, Reward mean: 117.51959431997, Reward std: 157.05870290041733\n",
      "Step: 300000, Reward mean: -67.08156257541543, Reward std: 84.02896712524053\n",
      "Step: 305000, Reward mean: 253.81561961306252, Reward std: 20.410929746714384\n",
      "Step: 310000, Reward mean: 95.61796819670937, Reward std: 157.84484161874244\n",
      "Step: 315000, Reward mean: 84.86824591042831, Reward std: 150.5611979137955\n",
      "Step: 320000, Reward mean: 205.15882872667277, Reward std: 62.73741813026878\n",
      "Step: 325000, Reward mean: 148.2019722245695, Reward std: 107.88731712357587\n",
      "Step: 330000, Reward mean: 79.14557462543489, Reward std: 168.84533352578381\n",
      "Step: 335000, Reward mean: 40.154461943439514, Reward std: 191.0257137470376\n",
      "Step: 340000, Reward mean: 80.27867200468329, Reward std: 154.16381339754363\n",
      "Step: 345000, Reward mean: 215.33669354344912, Reward std: 79.79677010628423\n",
      "Step: 350000, Reward mean: 265.25272345602974, Reward std: 22.874246754644478\n",
      "Step: 355000, Reward mean: -361.92129441255315, Reward std: 850.1682817617019\n",
      "Step: 360000, Reward mean: -59.52681080510538, Reward std: 529.1145034672642\n",
      "Step: 365000, Reward mean: 224.3060341443812, Reward std: 19.287372551847774\n",
      "Step: 370000, Reward mean: 112.1408564982061, Reward std: 131.46935067891064\n",
      "Step: 375000, Reward mean: 222.0540892084726, Reward std: 37.625631912152066\n",
      "Step: 380000, Reward mean: 33.001551824477204, Reward std: 193.12186052718232\n",
      "Step: 385000, Reward mean: 225.8581548928979, Reward std: 44.178018191063906\n",
      "Step: 390000, Reward mean: 247.43396619139384, Reward std: 26.02240605411461\n",
      "Step: 395000, Reward mean: 239.06553931040156, Reward std: 32.538400736049276\n",
      "Step: 400000, Reward mean: 210.6481614045399, Reward std: 58.920846286379515\n",
      "Step: 405000, Reward mean: 274.2184414316999, Reward std: 14.290870355353718\n",
      "Step: 410000, Reward mean: 256.71881859472734, Reward std: 14.448905332691279\n",
      "Step: 415000, Reward mean: 182.54558732085783, Reward std: 46.20303017563381\n",
      "Step: 420000, Reward mean: 103.65897039135055, Reward std: 138.3975804439671\n",
      "Step: 425000, Reward mean: 223.07764853019071, Reward std: 45.28292587798873\n",
      "Step: 430000, Reward mean: 168.5189307257116, Reward std: 114.81414473425252\n",
      "Step: 435000, Reward mean: -57.79782220272349, Reward std: 156.03012178476231\n",
      "Step: 440000, Reward mean: 282.2817058441407, Reward std: 22.755685147680644\n",
      "Step: 445000, Reward mean: 265.0970764453923, Reward std: 20.744979875321132\n",
      "Step: 450000, Reward mean: 153.2127317525988, Reward std: 94.19373691056612\n",
      "Step: 455000, Reward mean: 211.82817267232105, Reward std: 112.49146281423069\n",
      "Step: 460000, Reward mean: 205.62909509132413, Reward std: 106.15561010029309\n",
      "Step: 465000, Reward mean: 243.76502297415738, Reward std: 49.24483922607157\n",
      "Step: 470000, Reward mean: 255.686016584469, Reward std: 38.80423250039456\n",
      "Step: 475000, Reward mean: 238.7812308997789, Reward std: 53.145269625152395\n",
      "Step: 480000, Reward mean: 191.7752072443556, Reward std: 115.12759778346643\n",
      "Step: 485000, Reward mean: 166.41162845414493, Reward std: 103.52392944665526\n",
      "Step: 490000, Reward mean: 112.4383495488913, Reward std: 129.87505354884286\n",
      "Step: 495000, Reward mean: 87.26493596290908, Reward std: 145.72566908428146\n",
      "Step: 500000, Reward mean: 64.13257270919263, Reward std: 224.48197482832074\n"
     ]
    }
   ],
   "source": [
    "env = make(\"LunarLander-v2\")\n",
    "dqn = DQN(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
    "eps = 0.1\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(INITIAL_STEPS):\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    dqn.consume_transition((state, action, next_state, reward, done))\n",
    "\n",
    "    state = next_state if not done else env.reset()\n",
    "\n",
    "\n",
    "for i in range(TRANSITIONS):\n",
    "    #Epsilon-greedy policy\n",
    "    if random.random() < eps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = dqn.act(state)\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    dqn.update((state, action, next_state, reward, done))\n",
    "\n",
    "    state = next_state if not done else env.reset()\n",
    "\n",
    "    if (i + 1) % (TRANSITIONS//100) == 0:\n",
    "        rewards = evaluate_policy(dqn, 5)\n",
    "        print(f\"Step: {i+1}, Reward mean: {np.mean(rewards)}, Reward std: {np.std(rewards)}\")\n",
    "        dqn.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модели и визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Изменение режима для потока после его установки невозможно\n",
      "  warnings.warn(str(err))\n"
     ]
    }
   ],
   "source": [
    "# убрать комменты, если под линуксом, под виндой не запустилось(\n",
    "\n",
    "## загружаем модель\n",
    "# agent = Agent()\n",
    "## запуск агента\n",
    "# state = env.reset()\n",
    "# img = plt.imshow(env.render(mode='rgb_array'))\n",
    "# for _ in range(200):\n",
    "#     action = agent.act(state)\n",
    "#     img.set_data(env.render(mode='rgb_array')) \n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     display.display(plt.gcf())\n",
    "#     display.clear_output(wait=True)\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     if done:\n",
    "#         break\n",
    "        \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
